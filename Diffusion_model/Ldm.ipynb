{"cells":[{"cell_type":"markdown","metadata":{"id":"SADaMdswItdc"},"source":["#**Implementation of Latent Diffusion model**"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2957,"status":"ok","timestamp":1726567056420,"user":{"displayName":"Prerana bora","userId":"16728510100771523489"},"user_tz":-330},"id":"LogvkqDcBO1d","outputId":"fbf857d8-7d5b-457c-8ab9-9cc96e84e278"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1726567056420,"user":{"displayName":"Prerana bora","userId":"16728510100771523489"},"user_tz":-330},"id":"pxIwLVTsA3Go","outputId":"d409492e-a87e-4e35-f84d-548ab7c1e413"},"outputs":[{"output_type":"stream","name":"stdout","text":["File created at /content/drive/MyDrive/work_LDM/example.txt\n"]}],"source":["# Importing libraries\n","import numpy as np  # linear algebra\n","import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n","import os\n","\n","# Set the directory path to '/content' or another desired location\n","base_path = '/content/drive/MyDrive/base_LDM'  # Base path for storing or accessing files\n","\n","# List files in a specified directory (default: '/content')\n","for dirname, _, filenames in os.walk(base_path):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","# To access files in the 'working' directory, use '/content/your_directory_name'\n","working_directory = '/content/drive/MyDrive/work_LDM'\n","\n","os.makedirs(working_directory, exist_ok=True)\n","with open(os.path.join(working_directory, 'example.txt'), 'w') as f:\n","    f.write('This is an example file stored in the working directory of Google Colab.')\n","print(f\"File created at {os.path.join(working_directory, 'example.txt')}\")\n"]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3474,"status":"ok","timestamp":1726567059892,"user":{"displayName":"Prerana bora","userId":"16728510100771523489"},"user_tz":-330},"id":"L2bPw7SsBTAT","outputId":"76d125d8-0184-46d3-9274-82b99a8449fe"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: diffusers in /usr/local/lib/python3.10/dist-packages (0.30.3)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers) (8.5.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from diffusers) (3.16.0)\n","Requirement already satisfied: huggingface-hub>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from diffusers) (0.24.7)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from diffusers) (1.26.4)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from diffusers) (2024.5.15)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from diffusers) (2.32.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from diffusers) (0.4.5)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from diffusers) (9.4.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers) (2024.6.1)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers) (6.0.2)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers) (4.66.5)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers) (4.12.2)\n","Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers) (3.20.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (3.8)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (2024.8.30)\n"]}],"source":["!pip install diffusers"]},{"cell_type":"markdown","metadata":{"id":"qe-P1T7bXdhn"},"source":["#**Latent diffusion model:**\n","\n","Latent Diffusion Models (LDMs) are a specific type of diffusion model optimized for generating high-quality images in a more memory-efficient manner by operating in a lower-dimensional latent space. Unlike traditional diffusion models that work directly on image pixels and consume considerable memory, LDMs achieve the diffusion process in latent space, significantly reducing the memory requirements. This process involves training the model to denoise random Gaussian noise step by step to eventually produce an image.\n","\n","Main component:\n","\n","\n","*   CLIP Text Encoder: Converts input text into text embeddings.\n","*   Variational Auto Encoder (VAE): Compresses and decompresses images into and from a lower-dimensional latent space.\n","*   U-Net: Predicts the noise to be removed from the noised latent representations to reconstruct the original image data.\n","\n","**CLIP text Encoder**:\n","\n","The CLIP Text Encoder takes text as input and produces text embeddings. These embeddings represent the text in a form that is close in the latent space to the representation of images encoded by a similar process.\n","\n","\n","\n","*   Tokenization: Break down the input text into sub-words or tokens and convert these into numerical representations using a lookup table.\n","*   Text to Embedding Conversion: Utilize the CLIPTextModel to convert the numerical tokens into embeddings that encapsulate the semantic meaning of the text.\n","\n","*Role in Latent Diffusion*:\n","The text embeddings generated by the CLIP Text Encoder are used as one of the inputs to the U-Net model. This enables the generation of images that are semantically related to the input text.\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"jiy2l2zQYWiG"},"source":["#**Variational Auto Encoder (VAE)**\n","The VAE consists of two main parts: an encoder and a decoder. The encoder compresses an image into a lower-dimensional latent representation, and the decoder attempts to reconstruct the image from this latent representation.\n","\n","* Encoding: Compress an input image into a latent space representation.\n","\n","* Decoding: Reconstruct the image from the latent representation.\n","Role in Latent Diffusion\n","\n","The VAE is essential for reducing the computational load of the diffusion process. By operating in latent space, the diffusion model requires less computational power and memory, facilitating the generation of high-resolution images."]},{"cell_type":"markdown","metadata":{"id":"G6_xPPMRYhyu"},"source":["#**U-Net**\n","\n","The U-Net architecture takes two inputs: noisy latents and text embeddings. It outputs the predicted noise residuals to be subtracted from the noisy latents, effectively denoising them.\n","\n","* Adding Noise: Apply a series of noise levels to the latent representations according to a predetermined schedule.\n","\n","* Noise Prediction: For each noise level, predict the noise present in the noisy latents.\n","\n","* Denoising: Subtract the predicted noise from the noisy latents to move closer to the original image representation.\n","\n","*Role in Latent Diffusion*:\n","The U-Net is crucial for the iterative denoising process in latent diffusion. By gradually reducing noise from the latents, it guides the generation process to produce images that correspond to the textual description provided."]},{"cell_type":"markdown","metadata":{"id":"fvlZ0OSnBjTA"},"source":["**Importing libraries**\n"]},{"cell_type":"code","execution_count":33,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1726567059892,"user":{"displayName":"Prerana bora","userId":"16728510100771523489"},"user_tz":-330},"id":"PauB1V-yBiLf"},"outputs":[],"source":["import torch, logging\n","\n","## disable warnings\n","logging.disable(logging.WARNING)\n","\n","## Imaging  library\n","from PIL import Image\n","from torchvision import transforms as tfms\n","\n","## Basic libraries\n","import numpy as np\n","from tqdm.auto import tqdm\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","from IPython.display import display\n","import shutil\n","import os\n","\n","## For video display\n","from IPython.display import HTML\n","from base64 import b64encode\n","\n","\n","## Import the CLIP artifacts\n","from transformers import CLIPTextModel, CLIPTokenizer\n","from diffusers import AutoencoderKL, UNet2DConditionModel, LMSDiscreteScheduler\n","from IPython.display import display, clear_output\n","import os"]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1726567059892,"user":{"displayName":"Prerana bora","userId":"16728510100771523489"},"user_tz":-330},"id":"tvwciiw2oFbq","outputId":"7b88984c-8d0b-49a1-e864-f5aa574dd28c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Directory created or already exists: /content/drive/MyDrive/LDM/steps2\n"]}],"source":["import os\n","\n","# Define the directory path\n","steps_directory = '/content/drive/MyDrive/LDM/steps2'\n","\n","# Create the directory along with any intermediate directories if they don't exist\n","os.makedirs(steps_directory, exist_ok=True)\n","\n","print(f\"Directory created or already exists: {steps_directory}\")"]},{"cell_type":"markdown","metadata":{"id":"KLGaBTlSBwqi"},"source":["**setting cpu/gpu device**"]},{"cell_type":"code","execution_count":35,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1726567060430,"user":{"displayName":"Prerana bora","userId":"16728510100771523489"},"user_tz":-330},"id":"oQDDvs1tBtF5"},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"markdown","metadata":{"id":"oKuaZ_STCSmS"},"source":["**Load image**\n","\n","Loads an image from a specified path, converts it to RGB, and resizes it to a specified dimension.\n","\n","Parameters:\n","\n","* p (str): Path to the image file.\n","* size (tuple, optional): The dimensions to resize the image to. Default is (512, 512).\n","\n","Returns:\n","\n","* Image: An image object in RGB format with the specified dimensions."]},{"cell_type":"code","execution_count":36,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1726567062114,"user":{"displayName":"Prerana bora","userId":"16728510100771523489"},"user_tz":-330},"id":"SylsZGD8BtDC"},"outputs":[],"source":["def load_image(p):\n","    return Image.open(p).convert('RGB').resize((224,224))"]},{"cell_type":"markdown","metadata":{"id":"6yv9MS-3Cf3J"},"source":["**PIL to latent representation suitable for vae**\n","\n","Converts a PIL image to a latent representation suitable for input into a VAE model.\n","\n","Parameters:\n","\n","* image (PIL.Image): The image to convert.\n","\n","Returns:\n","\n","* Tensor: The latent representation of the image.\n"]},{"cell_type":"code","execution_count":37,"metadata":{"executionInfo":{"elapsed":651,"status":"ok","timestamp":1726567065887,"user":{"displayName":"Prerana bora","userId":"16728510100771523489"},"user_tz":-330},"id":"AWsrlDF6BtAT"},"outputs":[],"source":["def pil_to_latents(image):\n","    init_image = tfms.ToTensor()(image).unsqueeze(0) * 2.0 - 1.0\n","    init_image = init_image.to(device=\"cuda\", dtype=torch.float16)\n","    init_latent_dist = vae.encode(init_image).latent_dist.sample() * 0.18215\n","    return init_latent_dist"]},{"cell_type":"markdown","metadata":{"id":"dDOlbu6BCrup"},"source":["**Latent to PIL**\n","\n","Converts latents back into a PIL image, suitable for visualization and further processing.\n","\n","Parameters:\n","\n","* latents (Tensor): The latent representation to convert back to an image.\n","\n","Returns:\n","\n","* List[Image]: A list of image objects generated from the latent representations"]},{"cell_type":"code","execution_count":38,"metadata":{"executionInfo":{"elapsed":633,"status":"ok","timestamp":1726567068753,"user":{"displayName":"Prerana bora","userId":"16728510100771523489"},"user_tz":-330},"id":"QnbVialbBs9c"},"outputs":[],"source":["def latents_to_pil(latents):\n","    latents = (1 / 0.18215) * latents\n","    with torch.no_grad():\n","        image = vae.decode(latents).sample\n","    image = (image / 2 + 0.5).clamp(0, 1)\n","    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n","    images = (image * 255).round().astype(\"uint8\")\n","    pil_images = [Image.fromarray(image) for image in images]\n","    return pil_images"]},{"cell_type":"markdown","metadata":{"id":"rZ3yxIoXDERW"},"source":["**Text Encoder**\n","\n","Encodes textual prompts into embeddings using a CLIP text model.\n","Parameters:\n","\n","* prompts (List[str]): A list of textual prompts to encode.\n","* maxlen (int, optional): Maximum length of the encoded text. Defaults to the model's maximum length.\n","\n","Returns:\n","\n","* Tensor: The encoded text embeddings."]},{"cell_type":"code","execution_count":39,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1726567069822,"user":{"displayName":"Prerana bora","userId":"16728510100771523489"},"user_tz":-330},"id":"Yo64JTGXBs6s"},"outputs":[],"source":["def text_enc(prompts, maxlen=None):\n","    if maxlen is None: maxlen = tokenizer.model_max_length\n","    inp = tokenizer(prompts, padding=\"max_length\", max_length=maxlen, truncation=True, return_tensors=\"pt\")\n","    return text_encoder(inp.input_ids.to(\"cuda\"))[0].half()"]},{"cell_type":"markdown","metadata":{"id":"hdpYHhdMD_Lw"},"source":["**Prompt to image**\n","\n","Converts text prompts into images using a latent diffusion model.\n","\n","Parameters:\n","\n","* prompts (List[str]): Text prompts to convert into images.\n","* g (float): Guidance scale. Higher values enforce stronger adherence to the text prompt.\n","* seed (int): Random seed for generating images.\n","* steps (int): Number of diffusion steps.\n","* dim (int): Dimension of the generated images.\n","* save_int (bool): Whether to save intermediate images.\n","\n","Returns:\n","\n","* List[Image]: A list of generated image objects corresponding to the text prompts."]},{"cell_type":"markdown","metadata":{"id":"DaLpKjKkbN7r"},"source":["**Note**: Due to resource constraints here only 10 diffusion steps and 64 dimensions is given."]},{"cell_type":"code","execution_count":41,"metadata":{"executionInfo":{"elapsed":637,"status":"ok","timestamp":1726567078616,"user":{"displayName":"Prerana bora","userId":"16728510100771523489"},"user_tz":-330},"id":"g4OW263wodIe"},"outputs":[],"source":["def prompt_2_img(prompts, g=7.5, seed=100, steps=70, dim=512, save_int=True): #steps=70, dim=512 for optimal result\n","\n","    # Defining batch size\n","    bs = len(prompts)\n","\n","    # Converting textual prompts to embedding\n","    text = text_enc(prompts)\n","\n","    # Adding an unconditional prompt , helps in the generation process\n","    uncond =  text_enc([\"\"] * bs, text.shape[1])\n","    emb = torch.cat([uncond, text])\n","\n","    # Setting the seed\n","    if seed: torch.manual_seed(seed)\n","\n","    # Initiating random noise\n","    latents = torch.randn((bs, unet.in_channels, dim//8, dim//8))\n","\n","    # Setting number of steps in scheduler\n","    scheduler.set_timesteps(steps)\n","\n","    # Adding noise to the latents\n","    latents = latents.to(\"cuda\").half() * scheduler.init_noise_sigma\n","\n","    print(\"Processing text prompts:\", prompts)\n","    # Just before the loop starts:\n","    print(\"Visualizing initial latents...\")\n","    latents_norm = torch.norm(latents.view(latents.shape[0], -1), dim=1).mean().item()\n","    print(f\"Initial Latents Norm: {latents_norm}\")\n","\n","    # Iterating through defined steps\n","    for i,ts in enumerate(tqdm(scheduler.timesteps)):\n","        # We need to scale the i/p latents to match the variance\n","        inp = scheduler.scale_model_input(torch.cat([latents] * 2), ts)\n","\n","        # Predicting noise residual using U-Net\n","        with torch.no_grad(): u,t = unet(inp, ts, encoder_hidden_states=emb).sample.chunk(2)\n","\n","        # Performing Guidance\n","        pred = u + g*(t-u)\n","\n","        # Conditioning  the latents\n","        latents = scheduler.step(pred, ts, latents).prev_sample\n","\n","        # Inside your loop, after `latents` have been updated:\n","        latents_norm = torch.norm(latents.view(latents.shape[0], -1), dim=1).mean().item()\n","        print(f\"Step {i+1}/{steps} Latents Norm: {latents_norm}\")\n","\n","        from IPython.display import display, clear_output\n","        if save_int and i%10==0:\n","            !mkdir -p steps2 # Creating the directory if it doesn't exist\n","            image_path = f'steps2/la_{i:04d}.jpeg'\n","            latents_to_pil(latents)[0].save(image_path)\n","            display(latents_to_pil(latents)[0])  # Display the new image\n","\n","    return latents_to_pil(latents)"]},{"cell_type":"markdown","metadata":{"id":"voJFaew1bjX0"},"source":["**The Diffusion Process**\n","\n","* The stable diffusion model takes the textual input and a seed.\n","* The textual input is then passed through the CLIP model to generate textual embedding of size 77x768 and the seed is used to generate Gaussian noise of size 4x64x64 which becomes the first latent image representation.\n","* Next, the U-Net iteratively denoises the random latent image representations while conditioning on the text embeddings.\n","* The output of the U-Net is predicted noise residual, which is then used to compute conditioned latents via a scheduler algorithm.\n","* This process of denoising and text conditioning is repeated N times (We will use 2 as timestep due to gpu constraint) to retrieve a better latent image representation.\n","* Once this process is complete, the latent image representation (4x64x64) is decoded by the VAE decoder to retrieve the final output image (3x512x512)."]},{"cell_type":"code","execution_count":42,"metadata":{"executionInfo":{"elapsed":3375,"status":"ok","timestamp":1726567084840,"user":{"displayName":"Prerana bora","userId":"16728510100771523489"},"user_tz":-330},"id":"jbDhwJ6YojpJ"},"outputs":[],"source":["## Initiating tokenizer and encoder.\n","tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16)\n","text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16).to(\"cuda\")\n","\n","## Initiating the VAE\n","vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\", torch_dtype=torch.float16).to(\"cuda\")\n","\n","## Initializing a scheduler and Setting number of sampling steps\n","scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=5)\n","scheduler.set_timesteps(100)  # time step should be 50\n","\n","## Initializing the U-Net model\n","unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\", torch_dtype=torch.float16).to(\"cuda\")"]},{"cell_type":"markdown","metadata":{"id":"Sw8oZQHCcH5F"},"source":["**Result**"]},{"cell_type":"code","source":["images = prompt_2_img([\"A cat\"], save_int=True)\n","for img in images:display(img)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["837c30859c4c45dd84e750c95cd0c432","f8642cbc74fe48a284aaef2a2602db58","423959fb10d94b9383834fdecbc1311b","f4929602ee894c13909be21c17356efa","f49a20ac5e404957be281a47819f25c9","ea04aaa416a74d25821f06abca4e04de","dfc68eab11b84b78be7310927645074e","61049d93da374d2f90deadd8c7ef3af8","1a75d97a5afb4698a45b481f74ed97af","ced502e657f44f3c91b378ca51423f30","36a500f915ae4aacb643e00628e68e0a"],"output_embedded_package_id":"1f1OBuNIkHXBPUmdw6M78fZli-GOgR_Vu"},"id":"VYRT0dgKagPz","executionInfo":{"status":"ok","timestamp":1726567134667,"user_tz":-330,"elapsed":18033,"user":{"displayName":"Prerana bora","userId":"16728510100771523489"}},"outputId":"8e868ca7-65ca-473c-8458-5efb8692e962"},"execution_count":43,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"18-4Xz19p_MpjSWgje9-CCY6Yh5y0_8L_","referenced_widgets":["3aa30f1c57194c0dadba90335b2f30bd","4a91a137cf604220b07c267ee98f795c","2cff84ff98234d74a1527e21dddcffd5","92930e1510b3489a98de5c3566adfafd","227a56fd5663406097d5b7da4aeecda9","11e0b71aa99240da956aa7e990e767ff","d7a138adf6ab4ef7a14e5f065aa93134","fda625b85fef47379b77498550d00923","1665478ce2a8406bae0eb36057a5d71d","14f5f2678b4d4cf8a4a882ab9e05195d","78eb8e9090c749f3b9664bb7c379f88f"]},"executionInfo":{"elapsed":28219,"status":"ok","timestamp":1726564577187,"user":{"displayName":"Prerana bora","userId":"16728510100771523489"},"user_tz":-330},"id":"ZlE-ShH4pc9n","outputId":"4adb0175-a6f6-4982-c416-c06b6332b914"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["images = prompt_2_img([\"A cat\"], save_int=True)\n","for img in images:display(img)"]},{"cell_type":"markdown","metadata":{"id":"bj2h6aTfck4T"},"source":["**Result is not impressive as diffusion step is only given 100 and dimension is 512, also denoising step is given as 70 which is less in contrast of optimal denoising step. Noted: LDM is a resource hungry process**"]},{"cell_type":"code","source":[],"metadata":{"id":"oQkb23vc2FoA"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyOvLrl24rWEt6e0OR5EqMJ/"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"837c30859c4c45dd84e750c95cd0c432":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f8642cbc74fe48a284aaef2a2602db58","IPY_MODEL_423959fb10d94b9383834fdecbc1311b","IPY_MODEL_f4929602ee894c13909be21c17356efa"],"layout":"IPY_MODEL_f49a20ac5e404957be281a47819f25c9"}},"f8642cbc74fe48a284aaef2a2602db58":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ea04aaa416a74d25821f06abca4e04de","placeholder":"​","style":"IPY_MODEL_dfc68eab11b84b78be7310927645074e","value":"100%"}},"423959fb10d94b9383834fdecbc1311b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_61049d93da374d2f90deadd8c7ef3af8","max":70,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1a75d97a5afb4698a45b481f74ed97af","value":70}},"f4929602ee894c13909be21c17356efa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ced502e657f44f3c91b378ca51423f30","placeholder":"​","style":"IPY_MODEL_36a500f915ae4aacb643e00628e68e0a","value":" 70/70 [00:14&lt;00:00,  6.56it/s]"}},"f49a20ac5e404957be281a47819f25c9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ea04aaa416a74d25821f06abca4e04de":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dfc68eab11b84b78be7310927645074e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"61049d93da374d2f90deadd8c7ef3af8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1a75d97a5afb4698a45b481f74ed97af":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ced502e657f44f3c91b378ca51423f30":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"36a500f915ae4aacb643e00628e68e0a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3aa30f1c57194c0dadba90335b2f30bd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4a91a137cf604220b07c267ee98f795c","IPY_MODEL_2cff84ff98234d74a1527e21dddcffd5","IPY_MODEL_92930e1510b3489a98de5c3566adfafd"],"layout":"IPY_MODEL_227a56fd5663406097d5b7da4aeecda9"}},"4a91a137cf604220b07c267ee98f795c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_11e0b71aa99240da956aa7e990e767ff","placeholder":"​","style":"IPY_MODEL_d7a138adf6ab4ef7a14e5f065aa93134","value":"100%"}},"2cff84ff98234d74a1527e21dddcffd5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fda625b85fef47379b77498550d00923","max":100,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1665478ce2a8406bae0eb36057a5d71d","value":100}},"92930e1510b3489a98de5c3566adfafd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_14f5f2678b4d4cf8a4a882ab9e05195d","placeholder":"​","style":"IPY_MODEL_78eb8e9090c749f3b9664bb7c379f88f","value":" 100/100 [00:21&lt;00:00,  6.90it/s]"}},"227a56fd5663406097d5b7da4aeecda9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"11e0b71aa99240da956aa7e990e767ff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d7a138adf6ab4ef7a14e5f065aa93134":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fda625b85fef47379b77498550d00923":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1665478ce2a8406bae0eb36057a5d71d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"14f5f2678b4d4cf8a4a882ab9e05195d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"78eb8e9090c749f3b9664bb7c379f88f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}